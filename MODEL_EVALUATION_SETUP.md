# モデル評価機能のセットアップガイド

このガイドでは、訓練済みモデルを新しいデータで評価する機能のセットアップ手順を説明します。

## 🎯 機能概要

- ✅ **S3経由の統一処理**: すべてのデータをS3にアップロード
- ✅ **自動データ削除**: 一時データは24時間、結果は7日で自動削除（S3ライフサイクル）
- ✅ **SageMaker Processing**: 大量データも高速評価
- ✅ **詳細な結果**: 精度、混同行列、クラス別メトリクス、ファイル別予測

## 📐 アーキテクチャ

```
┌─────────────────────────────────────────────────────────┐
│ フロントエンド (React)                                   │
│                                                         │
│ 1. モデル選択 (S3から)                                   │
│ 2. テストデータ選択 (ローカルフォルダ)                    │
│ 3. メタデータ設定 (訓練時と同じ)                         │
│ 4. データをS3にアップロード                               │
│ 5. SageMaker Processing Job起動                         │
└─────────────────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────────────────┐
│ AWS バックエンド                                         │
│                                                         │
│ Lambda: start-evaluation                                │
│  → SageMaker Processing Jobを起動                       │
│                                                         │
│ SageMaker Processing Job                                │
│  → evaluate.py スクリプト実行                            │
│  → TFJSモデル読み込み                                    │
│  → 全ファイルを推論                                      │
│  → 精度・混同行列を計算                                  │
│  → 結果をS3に保存                                        │
│                                                         │
│ Lambda: get-evaluation-status                           │
│  → ジョブステータスを確認                                │
└─────────────────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────────────────┐
│ S3ストレージ (自動削除付き)                               │
│                                                         │
│ evaluation/temp/              → 24時間で削除            │
│   └── user123/                                          │
│       └── eval-20260108/      (アップロードデータ)      │
│                                                         │
│ evaluation/results/           → 7日で削除               │
│   └── user123/                                          │
│       └── audio-eval-xxx/     (評価結果)               │
│           ├── metrics.json                              │
│           └── predictions.csv                           │
└─────────────────────────────────────────────────────────┘
```

## 🚀 セットアップ手順

### 前提条件

- AWS アカウント
- AWS CLI がインストール済み
- Node.js 18以上
- 既存の Audio ML Studio がセットアップ済み

### 1. 評価スクリプトをS3にアップロード

#### Windows の場合:

```powershell
cd audio-augmentation-app\sagemaker-scripts
.\upload-evaluation-script.bat <your-bucket-name> ap-northeast-1
```

#### Mac/Linux の場合:

```bash
cd audio-augmentation-app/sagemaker-scripts
chmod +x upload-evaluation-script.sh
./upload-evaluation-script.sh <your-bucket-name> ap-northeast-1
```

これにより以下がアップロードされます:
- `s3://<bucket>/public/scripts/evaluation/evaluate.py`
- `s3://<bucket>/public/scripts/evaluation/requirements.txt`

### 2. バックエンドをデプロイ

```bash
cd audio-augmentation-app

# サンドボックス環境で確認
npx ampx sandbox

# または本番環境にデプロイ
npx ampx pipeline-deploy --branch main
```

デプロイ内容:
- ✅ S3ライフサイクルポリシー（自動削除）
- ✅ Lambda関数: `start-evaluation`
- ✅ Lambda関数: `get-evaluation-status`
- ✅ S3パス: `evaluation/temp/`, `evaluation/results/`

### 3. フロントエンドを起動

```bash
npm run dev
```

ブラウザで開いて、新しいタブ「モデル評価」が表示されることを確認。

## 📝 使い方

### ステップ1: モデルを選択

1. 「モデル評価」タブを開く
2. 保存済みモデルの一覧から評価したいモデルを選択

### ステップ2: テストデータを選択

1. 「フォルダを選択」ボタンをクリック
2. 評価したいWAVファイルが入ったフォルダを選択
3. 自動的にファイル数がカウントされる

### ステップ3: メタデータ設定

1. ファイル名からメタデータフィールドが自動抽出される
2. **訓練時と同じ設定を選択**:
   - ターゲットフィールド（予測対象）
   - 補助フィールド（追加情報）
3. 設定が正しいことを確認

### ステップ4: 評価実行

1. 「評価開始」ボタンをクリック
2. データが自動的にS3にアップロード（進捗表示あり）
3. SageMaker Processing Jobが起動
4. ステータスが自動更新（5-15分）

### ステップ5: 結果確認

評価が完了すると、以下が表示されます:

#### 全体メトリクス
- **精度 (Accuracy)**: 全体の正解率
- **F1スコア**: 適合率と再現率の調和平均
- **適合率 (Precision)**: 予測が正しい割合
- **再現率 (Recall)**: 実際の正解を捉えた割合

#### 混同行列 (Confusion Matrix)
- 縦軸: 実際のクラス
- 横軸: 予測されたクラス
- 対角線（緑）: 正解
- 非対角線（赤）: 誤り

#### クラス別の性能
各クラスごとの詳細指標

#### ファイル別の予測結果
個別のファイルごとの予測と信頼度

## 💾 結果のダウンロード

評価結果は以下のS3パスに保存されます:

```
evaluation/results/<userId>/<jobName>/
├── metrics.json          # 評価メトリクス
└── predictions.csv       # ファイル別の予測結果
```

これらは **7日後に自動削除** されます。

必要に応じて、S3コンソールまたはAWS CLIで手動ダウンロード可能:

```bash
aws s3 cp s3://<bucket>/evaluation/results/<userId>/<jobName>/ ./results/ --recursive
```

## 🔧 トラブルシューティング

### Q: 評価ジョブが起動しない

**確認事項:**
1. SageMaker IAM ロールに必要な権限があるか
2. 評価スクリプトが正しくS3にアップロードされているか
   ```bash
   aws s3 ls s3://<bucket>/public/scripts/evaluation/
   ```
3. Lambda関数がデプロイされているか

### Q: 評価中にエラーが発生

**CloudWatch Logsで確認:**
```bash
aws logs describe-log-groups --log-group-name-prefix /aws/sagemaker/ProcessingJobs
```

**よくあるエラー:**
- メタデータ設定が訓練時と異なる → 訓練時の設定を確認
- クラス名が一致しない → ファイル名のフォーマットを確認
- メモリ不足 → SageMakerインスタンスサイズを変更

### Q: 結果がダウンロードできない

**確認事項:**
1. 評価ジョブが正常に完了しているか
2. S3 バケットの CORS 設定
3. Amplify Storage のアクセス権限

### Q: 一時データが削除されない

**S3ライフサイクルポリシーを確認:**
```bash
aws s3api get-bucket-lifecycle-configuration --bucket <bucket-name>
```

手動削除:
```bash
aws s3 rm s3://<bucket>/evaluation/temp/ --recursive
```

## 💰 コスト目安

| リソース | 課金 | 概算コスト |
|---------|------|-----------|
| S3ストレージ | 保存量 + リクエスト | ~$0.01/GB/月 |
| S3データ転送 | アップロード/ダウンロード | アップロードは無料 |
| Lambda | 実行時間 | ほぼ無料（短時間） |
| **SageMaker Processing** | **実行時間のみ** | **$0.269/時間** (ml.m5.xlarge) |

### 例: 500ファイルの評価（5-10分）

**約 $0.02 ～ $0.04 / 回**

訓練よりも低コスト（CPUインスタンスのため）

## 📊 評価結果の見方

### 精度指標の解釈

- **Accuracy > 0.90**: 優秀
- **Accuracy 0.80-0.90**: 良好
- **Accuracy < 0.80**: 改善の余地あり

### F1スコアの解釈

- **F1 > 0.90**: 優秀
- **F1 0.80-0.90**: 良好
- **F1 < 0.80**: 不均衡データの可能性

### 混同行列から分かること

- **対角線が濃い**: よく分類されている
- **特定の組み合わせが濃い**: クラス間の混同がある
- **例**: 200kPa と 195kPa が混同 → より細かい特徴量が必要

### クラス別メトリクスから分かること

- **Precision低い**: 誤検出が多い（偽陽性）
- **Recall低い**: 見逃しが多い（偽陰性）
- **Support**: そのクラスのサンプル数

## 🎓 ベストプラクティス

### 1. 訓練データと評価データを分ける

✅ **正しい例:**
- 訓練: 2023年のデータ
- 評価: 2024年のデータ

❌ **間違った例:**
- 訓練と評価で同じデータを使う

### 2. メタデータ設定は訓練時と同じにする

訓練時:
```
ターゲット: フィールド0 (空気圧)
補助: フィールド2 (タイヤサイズ)
```

評価時も **同じ設定** を使用。

### 3. クラス分布を確認

訓練データと評価データでクラス分布が大きく異なると、精度が低下する可能性があります。

### 4. 複数の評価を実施

- 異なる時期のデータ
- 異なる条件のデータ
- 異なる録音環境のデータ

複数の評価結果を比較して、モデルの汎化性能を確認。

## 🔄 ワークフロー例

### 通常のML開発フロー

```
1. データ収集
   ↓
2. データ拡張 (Data Augmentation)
   ↓
3. モデル訓練 (Model Training)
   ↓
4. 訓練データで性能確認
   ↓
5. 【新機能】別データで評価 (Model Evaluation) ← ここ！
   ↓
6. 問題があれば 2 or 3 に戻る
   ↓
7. デプロイ
```

### 本番運用時

```
定期的に新しいデータで評価
  ↓
性能が低下していないか確認
  ↓
必要に応じて再訓練
```

## 📚 関連ドキュメント

- [CLOUD_TRAINING_SETUP.md](./CLOUD_TRAINING_SETUP.md) - クラウド訓練のセットアップ
- [README.md](./README.md) - プロジェクト全体の説明

## 🆘 サポート

問題が発生した場合:

1. CloudWatch Logsを確認
2. S3バケットの内容を確認
3. Lambda関数のログを確認
4. SageMaker Processing Jobのステータスを確認

---

これで、訓練したモデルの実力を正確に評価できます！🎉

